@misc{wang2024poodlepooleddenseselfsupervised,
      abbr={ICLR},
      title={PooDLe: Pooled and Dense Self-supervised Learning from Naturalistic Videos}, 
      author={Wang*, Alex N. and Hoang*, Christopher and Xiong, Yuwen and LeCun, Yann and Ren, Mengye},
      year={2025},
      eprint={2408.11208},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      arxiv={2408.11208},
      url={https://arxiv.org/abs/2408.11208},
      website={https://poodle-ssl.github.io/},
}

@article{orhan2024self,
  abbr=       {CogSci},
  title={Self-supervised Learning of Video Representations from a Child's Perspective},
  author={Orhan, A Emin and Wang, Wentao and Wang, Alex N. and Ren, Mengye and Lake, Brenden M},
  booktitle = 	 {Proceedings of the Annual Meeting of the Cognitive Science Society},
  year = 	 {2024},
  arxiv=      {2402.00300},
  code={https://github.com/eminorhan/video-models},
  abs={Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.},
}

@InProceedings{MetaLearn BatchNorm,
  abbr=       {NeurIPSw},

  title = 	 {Studying BatchNorm Learning Rate Decay on Meta-Learning Inner-Loop Adaptation},
  author =       {Wang*, Alex N. and Leung*, Gary and Doubov*, Sasha},
  booktitle = 	 {MetaLearn Workshop, Neural Information Processing Systems},
  year = 	 {2021},
  pdf = 	 {https://openreview.net/pdf?id=k9l1KkV4eQc},
  url = 	 {https://openreview.net/forum?id=k9l1KkV4eQc},
  abstract = 	 {Meta-learning for few-shot classification has been challenged on its effectiveness compared to simpler pretraining methods and the validity of its claim of "learning to learn". Recent work has suggested that MAML-based models do not perform "rapid-learning" in the inner-loop but reuse features by only adapting the final linear layer. Separately, BatchNorm, a near ubiquitous inclusion in model architectures, has been shown to have an implicit learning rate decay effect on the preceding layers of a network. We study the impact of BatchNorm's implicit learning rate decay on feature reuse in meta-learning methods and find that counteracting it increases change in intermediate layers during adaptation. We also find that counteracting this learning rate decay sometimes improves performance on few-shot classification tasks.},
}

@InProceedings{SketchEmbedNet,
  abbr=       {ICML},
  arxiv=      {2009.04806},
  code=       {https://github.com/alexnwang/SketchEmbedNet-public},

  title = 	 {SketchEmbedNet: Learning Novel Concepts by Imitating Drawings},
  author =       {Wang*, Alex N. and Ren*, Mengye and Zemel, Richard},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10870--10881},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wang21s/wang21s.pdf},
  url = 	 {http://proceedings.mlr.press/v139/wang21s.html},
  abstract = 	 {Sketch drawings capture the salient information of visual concepts. Previous work has shown that neural networks are capable of producing sketches of natural objects drawn from a small number of classes. While earlier approaches focus on generation quality or retrieval, we explore properties of image representations learned by training a model to produce sketches of images. We show that this generative, class-agnostic model produces informative embeddings of images from novel examples, classes, and even novel datasets in a few-shot setting. Additionally, we find that these learned representations exhibit interesting structure and compositionality.}
}

